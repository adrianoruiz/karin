### Resumo r√°pido do problema

Hoje o **DebounceManager** est√° agregando todas as mensagens hist√≥ricas do chat quando monta o prompt para o GPT. Isso faz com que o modelo ‚Äúre-responda‚Äù aquilo que j√° foi respondido, gerando repeti√ß√£o e mensagens enormes.
O comportamento correto deveria ser:

1. **Durante o intervalo do debounce (7 s)** ‚Äì concatenar tudo o que chegar para o mesmo `chatId`.
2. **Quando o timer dispara** ‚Äì enviar **apenas** esse bloco para o GPT, receber a resposta, limpar o buffer e arquivar a conversa normalmente.
3. **Mensagens que chegarem depois disso** iniciam um **novo** buffer; o GPT ver√° o hist√≥rico (para contexto), mas n√£o receber√° o mesmo texto duplicado.

---

## Pontos para o dev revisar

| Onde olhar                                                                           | O que conferir                                                                                       | Ajuste sugerido                                                                                                                                                                                                                                                                                                                |
| ------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **onFlush(chatId, messagesArray)** (camada de orquestra√ß√£o acima do DebounceManager) | Se est√° montando o prompt somando `messagesArray` **+ alguma cole√ß√£o global** que nunca √© esvaziada. | Na hora de criar o prompt: <br>`js<br>// N√ÉO fa√ßa issoüëá<br>prompt = allPreviousMessages.concat(messagesArray)‚Ä¶<br><br>// Fa√ßa assimüëá<br>prompt = systemContext.concat(messagesArray);<br>`<br>O `systemContext` pode trazer regras fixas e, se quiser, **um resumo** do hist√≥rico ‚Äì mas **n√£o** o texto bruto j√° respondido. |
| **timer callback** no DebounceManager                                                | Ap√≥s chamar `onFlush`, o buffer precisa sumir de fato.                                               | J√° existem chamadas a `memoryBuffer.delete()` e `redisClient.del()`. Verifique nos logs se elas est√£o executando. Se algum erro impedir a exclus√£o, force um `finally` para limpar mesmo em caso de falha.                                                                                                                     |
| **downloadMediaIfExists**                                                            | Pode aumentar muito o prompt se o Base64 da m√≠dia for mandado ao GPT.                                | N√£o envie o Base64 inteiro. Guarde s√≥ um placeholder ou o link para o arquivo.                                                                                                                                                                                                                                                 |
| **Fun√ß√£o que cria a requisi√ß√£o GPT**                                                 | Garantir deduplica√ß√£o por **ID da mensagem**.                                                        | Se precisar manter o hist√≥rico completo, armazene `<chatId, messageId>` numa tabela/cole√ß√£o ‚Äúj√° enviado‚Äù. Antes de mandar algo novo, fa√ßa um `filter` removendo IDs que j√° foram enviados.                                                                                                                                     |
| **Tamanho do contexto**                                                              | O prompt est√° crescendo indefinidamente?                                                             | Considere resumir o hist√≥rico antigo com embeddings ou ‚Äúconversation summary‚Äù em vez de concatenar tudo.                                                                                                                                                                                                                       |

---

## Passo-a-passo bem objetivo

1. **Limpe o buffer ap√≥s processar**

   * J√° temos `memoryBuffer.delete(chatId)` e `await redisClient.del(key)`; garanta que esses comandos n√£o estejam dentro de um bloco que possa ser pulado por exce√ß√£o.
   * Adicione um *log* **INFO** confirmando a remo√ß√£o (‚ÄúBuffer zerado para chat X‚Äù).

2. **Monte o prompt s√≥ com o lote atual**

   ```js
   const prompt = [
     { role: 'system', content: systemContext },
     ...messagesArray.map(m => ({ role: 'user', content: m.body }))
   ];
   ```

   > ‚ö†Ô∏è **N√£o** inclua respostas anteriores da pr√≥pria IA.

3. **Guarde contexto sem repetir texto**

   * Ap√≥s receber a `assistant` reply, grave **apenas**: `{role:'assistant', content: resposta}`
   * Na pr√≥xima rodada, se quiser contexto, traga **s√≥** esses objetos; n√£o replique de novo a parte do usu√°rio que j√° foi enviada.

4. **Proteja contra falhas**

   * Se o `onFlush` lan√ßar erro, use `finally` para limpar buffer/timer mesmo assim, evitando ‚Äútimers √≥rf√£os‚Äù e dados presos.

```js
try {
  await onFlush(chatId, finalBufferToFlush);
} catch (e) {
  logger.error('Falha no onFlush', e);
} finally {
  await clearBuffers(chatId); // fun√ß√£o que deleta Redis/memory e timer
}
```

5. **Teste**

   * Envie tr√™s mensagens r√°pidas (menos de 7 s) ‚Üí verifique que o GPT recebe **apenas um** prompt com 3 mensagens.
   * Espere a resposta.
   * Envie outra mensagem ‚Üí o GPT deve responder **s√≥** a esta nova pergunta, sem repetir a anterior.

---

### Frase curtinha para o commit

> ‚ÄúEvita prompt duplicado: zera buffer ap√≥s flush e envia s√≥ o lote do debounce.‚Äù

Com esses ajustes o bot continuar√° lembrando da conversa, mas **n√£o** vai somar respostas antigas no texto que ele devolve.
